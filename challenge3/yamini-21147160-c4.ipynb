{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import python libraries\nimport numpy as np\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, GRU, RNN\nfrom tensorflow.keras.utils import to_categorical\nfrom random import randint\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:59:37.451096Z","iopub.execute_input":"2023-03-26T20:59:37.451534Z","iopub.status.idle":"2023-03-26T20:59:47.401586Z","shell.execute_reply.started":"2023-03-26T20:59:37.451499Z","shell.execute_reply":"2023-03-26T20:59:47.400083Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import nltk   # natural language tool kit library\nnltk.download('gutenberg')  # downloads a library that NLTK uses\n\nfrom nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\nprint(gut.fileids())  \n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:59:47.403453Z","iopub.execute_input":"2023-03-26T20:59:47.404167Z","iopub.status.idle":"2023-03-26T20:59:48.594981Z","shell.execute_reply.started":"2023-03-26T20:59:47.404130Z","shell.execute_reply":"2023-03-26T20:59:48.592750Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package gutenberg to /usr/share/nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n","output_type":"stream"}]},{"cell_type":"code","source":"# get the book text\nbook_text = nltk.corpus.gutenberg.raw('blake-poems.txt')","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:59:51.870963Z","iopub.execute_input":"2023-03-26T20:59:51.871485Z","iopub.status.idle":"2023-03-26T20:59:51.882678Z","shell.execute_reply.started":"2023-03-26T20:59:51.871447Z","shell.execute_reply":"2023-03-26T20:59:51.880653Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# print the first 500 characters of the text so we can look at it\nprint(book_text[:500])","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:45:48.438169Z","iopub.execute_input":"2023-03-26T08:45:48.439224Z","iopub.status.idle":"2023-03-26T08:45:48.446436Z","shell.execute_reply.started":"2023-03-26T08:45:48.439163Z","shell.execute_reply":"2023-03-26T08:45:48.445084Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[Poems by William Blake 1789]\n\n \nSONGS OF INNOCENCE AND OF EXPERIENCE\nand THE BOOK of THEL\n\n\n SONGS OF INNOCENCE\n \n \n INTRODUCTION\n \n Piping down the valleys wild,\n   Piping songs of pleasant glee,\n On a cloud I saw a child,\n   And he laughing said to me:\n \n \"Pipe a song about a Lamb!\"\n   So I piped with merry cheer.\n \"Piper, pipe that song again;\"\n   So I piped: he wept to hear.\n \n \"Drop thy pipe, thy happy pipe;\n   Sing thy songs of happy cheer:!\"\n So I sang the same again,\n   While he wept wi\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence.lower()","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:01.991534Z","iopub.execute_input":"2023-03-26T21:00:01.992050Z","iopub.status.idle":"2023-03-26T21:00:02.001390Z","shell.execute_reply.started":"2023-03-26T21:00:01.992008Z","shell.execute_reply":"2023-03-26T21:00:01.999310Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"book_text = preprocess_text(book_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:14.607902Z","iopub.execute_input":"2023-03-26T21:00:14.608422Z","iopub.status.idle":"2023-03-26T21:00:14.619192Z","shell.execute_reply.started":"2023-03-26T21:00:14.608378Z","shell.execute_reply":"2023-03-26T21:00:14.617417Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(len(book_text))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:00.031060Z","iopub.execute_input":"2023-03-26T08:46:00.032540Z","iopub.status.idle":"2023-03-26T08:46:00.039571Z","shell.execute_reply.started":"2023-03-26T08:46:00.032482Z","shell.execute_reply":"2023-03-26T08:46:00.038110Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"34028\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n# punkt is a sentence tokenizer that nltk requires. \n# It divides a text into a list of sentences, by using an unsupervised algorithm \n# to build a model for abbreviation words, collocations, and words that start sentences\nnltk.download('punkt')\n\nbook_text_words = (word_tokenize(book_text))\nn_words = len(book_text_words)\nunique_words = len(set(book_text_words))\n\nprint('Total Words: %d' % n_words)\nprint('Unique Words: %d' % unique_words)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:20.787432Z","iopub.execute_input":"2023-03-26T21:00:20.788001Z","iopub.status.idle":"2023-03-26T21:00:20.926410Z","shell.execute_reply.started":"2023-03-26T21:00:20.787952Z","shell.execute_reply":"2023-03-26T21:00:20.923837Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nTotal Words: 6586\nUnique Words: 1504\n","output_type":"stream"}]},{"cell_type":"code","source":"# convert words to numbers\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=unique_words)\ntokenizer.fit_on_texts(book_text_words)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:24.614292Z","iopub.execute_input":"2023-03-26T21:00:24.614849Z","iopub.status.idle":"2023-03-26T21:00:24.676317Z","shell.execute_reply.started":"2023-03-26T21:00:24.614789Z","shell.execute_reply":"2023-03-26T21:00:24.675295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\nword_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:27.749768Z","iopub.execute_input":"2023-03-26T21:00:27.752083Z","iopub.status.idle":"2023-03-26T21:00:27.758090Z","shell.execute_reply.started":"2023-03-26T21:00:27.752020Z","shell.execute_reply":"2023-03-26T21:00:27.756336Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# just for exploration, let's print the 1000th word in the dictionary and it's index\nprint(book_text_words[1000])\nprint(word_2_index[book_text_words[500]])","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:14.741616Z","iopub.execute_input":"2023-03-26T08:46:14.742494Z","iopub.status.idle":"2023-03-26T08:46:14.752256Z","shell.execute_reply.started":"2023-03-26T08:46:14.742412Z","shell.execute_reply":"2023-03-26T08:46:14.749252Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"did\n363\n","output_type":"stream"}]},{"cell_type":"code","source":"input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\ninput_sequence = []   # empty list to hold the sequences that will be input into our model\noutput_words = []     # empty list to hold the output words\ninput_seq_length = 100  # length of the input sequence\n\n# form the input sequence list and the output words list\nfor i in range(0, n_words - input_seq_length , 1):\n    in_seq = book_text_words[i:i + input_seq_length]\n    input_sequence_words.append(in_seq)\n    out_seq = book_text_words[i + input_seq_length]\n    input_sequence.append([word_2_index[word] for word in in_seq])\n    output_words.append(word_2_index[out_seq])","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:39.678946Z","iopub.execute_input":"2023-03-26T21:00:39.679381Z","iopub.status.idle":"2023-03-26T21:00:39.777347Z","shell.execute_reply.started":"2023-03-26T21:00:39.679351Z","shell.execute_reply":"2023-03-26T21:00:39.776417Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# print the first sequence to see what it looks like - a list of 100 integers that represent the first observation of words\nprint(len(input_sequence))      # print the number of input sequences\nprint(input_sequence[0])        # print the first input sequence\nprint(len(input_sequence[0]))   # print the length of the first input sequence","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:19.605166Z","iopub.execute_input":"2023-03-26T08:46:19.605582Z","iopub.status.idle":"2023-03-26T08:46:19.612544Z","shell.execute_reply.started":"2023-03-26T08:46:19.605546Z","shell.execute_reply":"2023-03-26T08:46:19.610791Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"6490\n[709, 40, 459, 276, 164, 3, 347, 2, 3, 348, 2, 1, 189, 3, 63, 164, 3, 347, 460, 349, 49, 1, 277, 121, 349, 164, 3, 226, 710, 12, 73, 165, 54, 2, 9, 227, 143, 5, 21, 278, 112, 711, 74, 45, 461, 7, 122, 462, 463, 278, 16, 112, 350, 45, 461, 9, 123, 5, 64, 712, 27, 278, 27, 50, 278, 98, 27, 164, 3, 50, 462, 45, 464, 1, 713, 350, 81, 9, 123, 7, 37, 5, 64, 463, 89, 15, 49, 2, 714, 4, 189, 16, 17, 190, 465, 45, 9, 279, 25, 6]\n100\n","output_type":"stream"}]},{"cell_type":"code","source":"# reshape the input sequences to be 3-dimensional\n#X = np.reshape(input_sequence, (3562, 100, 1))    # number of input sequences, length of each sequence\nX = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n\n# Normalise the data by dividing by the max number of unique words (the vocab size)\nX = X / float(vocab_size)\n\n# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\ny = to_categorical(output_words)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:00:48.596878Z","iopub.execute_input":"2023-03-26T21:00:48.597665Z","iopub.status.idle":"2023-03-26T21:00:48.668015Z","shell.execute_reply.started":"2023-03-26T21:00:48.597612Z","shell.execute_reply":"2023-03-26T21:00:48.665788Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:31.459173Z","iopub.execute_input":"2023-03-26T08:46:31.459595Z","iopub.status.idle":"2023-03-26T08:46:31.467589Z","shell.execute_reply.started":"2023-03-26T08:46:31.459560Z","shell.execute_reply":"2023-03-26T08:46:31.465582Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"X shape: (6490, 100, 1)\ny shape: (6490, 1506)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Sequential()\n# LSTM layer has 800 neurons (units).  The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\nmodel.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(LSTM(800, return_sequences=True))\nmodel.add(LSTM(800))\nmodel.add(Dense(y.shape[1], activation='softmax'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:33.656013Z","iopub.execute_input":"2023-03-26T08:46:33.656501Z","iopub.status.idle":"2023-03-26T08:46:36.931769Z","shell.execute_reply.started":"2023-03-26T08:46:33.656463Z","shell.execute_reply":"2023-03-26T08:46:36.929832Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 100, 800)          2566400   \n                                                                 \n lstm_1 (LSTM)               (None, 100, 800)          5123200   \n                                                                 \n lstm_2 (LSTM)               (None, 800)               5123200   \n                                                                 \n dense (Dense)               (None, 1506)              1206306   \n                                                                 \n=================================================================\nTotal params: 14,019,106\nTrainable params: 14,019,106\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# the output word can be one of any of the unique words in the vocabulary\n# This means it is a multi-class calssification problem and we use the categorical crossentropy loss function\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:47.084041Z","iopub.execute_input":"2023-03-26T08:46:47.084533Z","iopub.status.idle":"2023-03-26T08:46:47.112487Z","shell.execute_reply.started":"2023-03-26T08:46:47.084481Z","shell.execute_reply":"2023-03-26T08:46:47.110700Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.fit(X, y, batch_size=50, epochs=100, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T08:46:51.129542Z","iopub.execute_input":"2023-03-26T08:46:51.131243Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n130/130 [==============================] - 905s 7s/step - loss: 6.5015\nEpoch 2/100\n130/130 [==============================] - 900s 7s/step - loss: 6.2230\nEpoch 3/100\n130/130 [==============================] - 919s 7s/step - loss: 6.1894\nEpoch 4/100\n130/130 [==============================] - 915s 7s/step - loss: 6.1806\nEpoch 5/100\n130/130 [==============================] - 929s 7s/step - loss: 6.1723\nEpoch 6/100\n130/130 [==============================] - 924s 7s/step - loss: 6.1696\nEpoch 7/100\n130/130 [==============================] - 823s 6s/step - loss: 6.1670\nEpoch 8/100\n130/130 [==============================] - 792s 6s/step - loss: 6.1659\nEpoch 9/100\n130/130 [==============================] - 785s 6s/step - loss: 6.1632\nEpoch 10/100\n130/130 [==============================] - 781s 6s/step - loss: 6.1606\nEpoch 11/100\n130/130 [==============================] - 788s 6s/step - loss: 6.1593\nEpoch 12/100\n130/130 [==============================] - 787s 6s/step - loss: 6.1590\nEpoch 13/100\n130/130 [==============================] - 805s 6s/step - loss: 6.1565\nEpoch 14/100\n130/130 [==============================] - 786s 6s/step - loss: 6.1557\nEpoch 15/100\n130/130 [==============================] - 785s 6s/step - loss: 6.1566\nEpoch 16/100\n130/130 [==============================] - 773s 6s/step - loss: 6.1559\nEpoch 17/100\n130/130 [==============================] - 771s 6s/step - loss: 6.1531\nEpoch 18/100\n130/130 [==============================] - 770s 6s/step - loss: 6.1534\nEpoch 19/100\n130/130 [==============================] - 778s 6s/step - loss: 6.1518\nEpoch 20/100\n130/130 [==============================] - 834s 6s/step - loss: 6.1533\nEpoch 21/100\n130/130 [==============================] - 855s 7s/step - loss: 6.1520\nEpoch 22/100\n130/130 [==============================] - 793s 6s/step - loss: 6.1498\nEpoch 23/100\n130/130 [==============================] - 872s 7s/step - loss: 6.1473\nEpoch 24/100\n130/130 [==============================] - 937s 7s/step - loss: 6.1429\nEpoch 25/100\n130/130 [==============================] - 927s 7s/step - loss: 6.1226\nEpoch 26/100\n130/130 [==============================] - 926s 7s/step - loss: 6.0985\nEpoch 27/100\n130/130 [==============================] - 926s 7s/step - loss: 6.0631\nEpoch 28/100\n130/130 [==============================] - 920s 7s/step - loss: 6.0294\nEpoch 29/100\n130/130 [==============================] - 942s 7s/step - loss: 6.0031\nEpoch 30/100\n130/130 [==============================] - 963s 7s/step - loss: 5.9755\nEpoch 31/100\n130/130 [==============================] - 965s 7s/step - loss: 5.9369\nEpoch 32/100\n130/130 [==============================] - 964s 7s/step - loss: 5.8946\nEpoch 33/100\n130/130 [==============================] - 940s 7s/step - loss: 5.8603\nEpoch 34/100\n130/130 [==============================] - 937s 7s/step - loss: 5.8259\nEpoch 35/100\n130/130 [==============================] - 939s 7s/step - loss: 5.7907\nEpoch 36/100\n130/130 [==============================] - 930s 7s/step - loss: 5.7555\nEpoch 37/100\n130/130 [==============================] - 932s 7s/step - loss: 5.7210\nEpoch 38/100\n130/130 [==============================] - 921s 7s/step - loss: 5.6929\nEpoch 39/100\n130/130 [==============================] - 923s 7s/step - loss: 5.6437\nEpoch 40/100\n130/130 [==============================] - 924s 7s/step - loss: 5.6126\nEpoch 41/100\n130/130 [==============================] - 923s 7s/step - loss: 5.5762\nEpoch 42/100\n130/130 [==============================] - 913s 7s/step - loss: 5.5298\nEpoch 43/100\n130/130 [==============================] - 918s 7s/step - loss: 5.4668\nEpoch 44/100\n130/130 [==============================] - 926s 7s/step - loss: 5.4189\nEpoch 45/100\n130/130 [==============================] - 921s 7s/step - loss: 5.3916\nEpoch 46/100\n130/130 [==============================] - 933s 7s/step - loss: 5.3613\nEpoch 47/100\n 62/130 [=============>................] - ETA: 8:12 - loss: 5.2554","output_type":"stream"}]},{"cell_type":"code","source":"model1 = Sequential()\n# LSTM layer has 800 neurons (units).  The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\nmodel1.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel1.add(LSTM(128, return_sequences=True))\nmodel1.add(LSTM(64))\nmodel1.add(Dense(y.shape[1], activation='softmax'))\n\nmodel1.summary()\n\n\n\nmodel1.compile(loss='categorical_crossentropy', optimizer='adam')\nprint(model1.summary())\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:05:06.609816Z","iopub.execute_input":"2023-03-26T21:05:06.610336Z","iopub.status.idle":"2023-03-26T21:05:07.659868Z","shell.execute_reply.started":"2023-03-26T21:05:06.610290Z","shell.execute_reply":"2023-03-26T21:05:07.657766Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 100, 128)          66560     \n                                                                 \n lstm_1 (LSTM)               (None, 100, 128)          131584    \n                                                                 \n lstm_2 (LSTM)               (None, 64)                49408     \n                                                                 \n dense (Dense)               (None, 1505)              97825     \n                                                                 \n=================================================================\nTotal params: 345,377\nTrainable params: 345,377\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 100, 128)          66560     \n                                                                 \n lstm_1 (LSTM)               (None, 100, 128)          131584    \n                                                                 \n lstm_2 (LSTM)               (None, 64)                49408     \n                                                                 \n dense (Dense)               (None, 1505)              97825     \n                                                                 \n=================================================================\nTotal params: 345,377\nTrainable params: 345,377\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"model1.fit(X, y, batch_size=128, epochs=20, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:06:07.499274Z","iopub.execute_input":"2023-03-26T21:06:07.499796Z","iopub.status.idle":"2023-03-26T21:13:52.400944Z","shell.execute_reply.started":"2023-03-26T21:06:07.499751Z","shell.execute_reply":"2023-03-26T21:13:52.399169Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/20\n51/51 [==============================] - 28s 441ms/step - loss: 6.7311\nEpoch 2/20\n51/51 [==============================] - 23s 442ms/step - loss: 6.1978\nEpoch 3/20\n51/51 [==============================] - 23s 458ms/step - loss: 6.1742\nEpoch 4/20\n51/51 [==============================] - 23s 445ms/step - loss: 6.1663\nEpoch 5/20\n51/51 [==============================] - 23s 452ms/step - loss: 6.1637\nEpoch 6/20\n51/51 [==============================] - 23s 450ms/step - loss: 6.1626\nEpoch 7/20\n51/51 [==============================] - 22s 437ms/step - loss: 6.1615\nEpoch 8/20\n51/51 [==============================] - 23s 444ms/step - loss: 6.1606\nEpoch 9/20\n51/51 [==============================] - 23s 447ms/step - loss: 6.1584\nEpoch 10/20\n51/51 [==============================] - 23s 451ms/step - loss: 6.1576\nEpoch 11/20\n51/51 [==============================] - 24s 465ms/step - loss: 6.1556\nEpoch 12/20\n51/51 [==============================] - 23s 452ms/step - loss: 6.1555\nEpoch 13/20\n51/51 [==============================] - 23s 446ms/step - loss: 6.1543\nEpoch 14/20\n51/51 [==============================] - 23s 451ms/step - loss: 6.1557\nEpoch 15/20\n51/51 [==============================] - 23s 444ms/step - loss: 6.1526\nEpoch 16/20\n51/51 [==============================] - 23s 455ms/step - loss: 6.1523\nEpoch 17/20\n51/51 [==============================] - 23s 452ms/step - loss: 6.1500\nEpoch 18/20\n51/51 [==============================] - 23s 445ms/step - loss: 6.1511\nEpoch 19/20\n51/51 [==============================] - 23s 459ms/step - loss: 6.1505\nEpoch 20/20\n51/51 [==============================] - 24s 473ms/step - loss: 6.1484\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fdfe0d03b50>"},"metadata":{}}]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model1.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n\n# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:14:27.570872Z","iopub.execute_input":"2023-03-26T21:14:27.571655Z","iopub.status.idle":"2023-03-26T21:14:40.966852Z","shell.execute_reply.started":"2023-03-26T21:14:27.571593Z","shell.execute_reply":"2023-03-26T21:14:40.965390Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"or think on him who bore thy name graze after thee and weep for washed in life river my bright mane for ever shall shine like the gold as guard er the fold spring sound the flute now it mute bird delight day and night nightingale in the dale lark in sky merrily merrily merrily to welcome in the year little boy full of joy little girl sweet and small cock does crow so do you merry voice infant noise merrily merrily to welcome in the year little lamb here am come and lick my white neck let me pull\nSeed word sequence: or think on him who bore thy name graze after thee and weep for washed in life river my bright mane for ever shall shine like the gold as guard er the fold spring sound the flute now it mute bird delight day and night nightingale in the dale lark in sky merrily merrily merrily to welcome in the year little boy full of joy little girl sweet and small cock does crow so do you merry voice infant noise merrily merrily to welcome in the year little lamb here am come and lick my white neck let me pull\nPredicted words: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\nBLEU Score for predicted words: 0.2384093783383091\n","output_type":"stream"}]},{"cell_type":"code","source":"model2 = Sequential()\n# LSTM layer has 800 neurons (units).  The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\nmodel2.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel2.add(LSTM(128, return_sequences=True))\nmodel2.add(LSTM(64))\nmodel2.add(Dense(y.shape[1], activation='softmax'))\n\nmodel2.summary()\n\n\n\nmodel2.compile(loss='categorical_crossentropy', optimizer='adam')\nprint(model2.summary())\nmodel2.fit(X, y, batch_size=128, epochs=50, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:16:00.908817Z","iopub.execute_input":"2023-03-26T21:16:00.910315Z","iopub.status.idle":"2023-03-26T21:36:14.574023Z","shell.execute_reply.started":"2023-03-26T21:16:00.910215Z","shell.execute_reply":"2023-03-26T21:36:14.572823Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_3 (LSTM)               (None, 100, 128)          66560     \n                                                                 \n lstm_4 (LSTM)               (None, 100, 128)          131584    \n                                                                 \n lstm_5 (LSTM)               (None, 64)                49408     \n                                                                 \n dense_1 (Dense)             (None, 1505)              97825     \n                                                                 \n=================================================================\nTotal params: 345,377\nTrainable params: 345,377\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_3 (LSTM)               (None, 100, 128)          66560     \n                                                                 \n lstm_4 (LSTM)               (None, 100, 128)          131584    \n                                                                 \n lstm_5 (LSTM)               (None, 64)                49408     \n                                                                 \n dense_1 (Dense)             (None, 1505)              97825     \n                                                                 \n=================================================================\nTotal params: 345,377\nTrainable params: 345,377\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/50\n51/51 [==============================] - 29s 467ms/step - loss: 6.7284\nEpoch 2/50\n51/51 [==============================] - 24s 465ms/step - loss: 6.1930\nEpoch 3/50\n51/51 [==============================] - 24s 467ms/step - loss: 6.1735\nEpoch 4/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1660\nEpoch 5/50\n51/51 [==============================] - 24s 475ms/step - loss: 6.1632\nEpoch 6/50\n51/51 [==============================] - 24s 468ms/step - loss: 6.1625\nEpoch 7/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1588\nEpoch 8/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1591\nEpoch 9/50\n51/51 [==============================] - 24s 463ms/step - loss: 6.1558\nEpoch 10/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1555\nEpoch 11/50\n51/51 [==============================] - 25s 482ms/step - loss: 6.1545\nEpoch 12/50\n51/51 [==============================] - 24s 474ms/step - loss: 6.1548\nEpoch 13/50\n51/51 [==============================] - 24s 470ms/step - loss: 6.1552\nEpoch 14/50\n51/51 [==============================] - 24s 461ms/step - loss: 6.1525\nEpoch 15/50\n51/51 [==============================] - 24s 472ms/step - loss: 6.1540\nEpoch 16/50\n51/51 [==============================] - 25s 481ms/step - loss: 6.1535\nEpoch 17/50\n51/51 [==============================] - 24s 464ms/step - loss: 6.1505\nEpoch 18/50\n51/51 [==============================] - 24s 467ms/step - loss: 6.1512\nEpoch 19/50\n51/51 [==============================] - 23s 459ms/step - loss: 6.1484\nEpoch 20/50\n51/51 [==============================] - 23s 460ms/step - loss: 6.1489\nEpoch 21/50\n51/51 [==============================] - 25s 490ms/step - loss: 6.1494\nEpoch 22/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1485\nEpoch 23/50\n51/51 [==============================] - 25s 481ms/step - loss: 6.1479\nEpoch 24/50\n51/51 [==============================] - 24s 465ms/step - loss: 6.1455\nEpoch 25/50\n51/51 [==============================] - 24s 465ms/step - loss: 6.1461\nEpoch 26/50\n51/51 [==============================] - 24s 467ms/step - loss: 6.1445\nEpoch 27/50\n51/51 [==============================] - 24s 462ms/step - loss: 6.1461\nEpoch 28/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1455\nEpoch 29/50\n51/51 [==============================] - 24s 478ms/step - loss: 6.1453\nEpoch 30/50\n51/51 [==============================] - 24s 461ms/step - loss: 6.1452\nEpoch 31/50\n51/51 [==============================] - 24s 470ms/step - loss: 6.1446\nEpoch 32/50\n51/51 [==============================] - 24s 464ms/step - loss: 6.1445\nEpoch 33/50\n51/51 [==============================] - 24s 471ms/step - loss: 6.1431\nEpoch 34/50\n51/51 [==============================] - 24s 472ms/step - loss: 6.1444\nEpoch 35/50\n51/51 [==============================] - 25s 487ms/step - loss: 6.1451\nEpoch 36/50\n51/51 [==============================] - 25s 487ms/step - loss: 6.1440\nEpoch 37/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1425\nEpoch 38/50\n51/51 [==============================] - 25s 481ms/step - loss: 6.1414\nEpoch 39/50\n51/51 [==============================] - 24s 470ms/step - loss: 6.1408\nEpoch 40/50\n51/51 [==============================] - 25s 481ms/step - loss: 6.1418\nEpoch 41/50\n51/51 [==============================] - 25s 490ms/step - loss: 6.1407\nEpoch 42/50\n51/51 [==============================] - 24s 478ms/step - loss: 6.1408\nEpoch 43/50\n51/51 [==============================] - 24s 479ms/step - loss: 6.1409\nEpoch 44/50\n51/51 [==============================] - 25s 481ms/step - loss: 6.1401\nEpoch 45/50\n51/51 [==============================] - 23s 459ms/step - loss: 6.1414\nEpoch 46/50\n51/51 [==============================] - 24s 473ms/step - loss: 6.1391\nEpoch 47/50\n51/51 [==============================] - 24s 478ms/step - loss: 6.1403\nEpoch 48/50\n51/51 [==============================] - 25s 497ms/step - loss: 6.1386\nEpoch 49/50\n51/51 [==============================] - 25s 497ms/step - loss: 6.1381\nEpoch 50/50\n51/51 [==============================] - 25s 487ms/step - loss: 6.1362\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fdfe129a250>"},"metadata":{}}]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model2.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n\n# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:41:20.111400Z","iopub.execute_input":"2023-03-26T21:41:20.112096Z","iopub.status.idle":"2023-03-26T21:41:33.389126Z","shell.execute_reply.started":"2023-03-26T21:41:20.112040Z","shell.execute_reply":"2023-03-26T21:41:33.387576Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"becomes an infant small he becomes man of woe he doth feel the sorrow too think not thou canst sigh sigh and thy maker is not by think not thou canst weep tear and thy maker is not year oh he gives to us his joy that our grief he may destroy till our grief is fled an gone he doth sit by us and moan songs of experience introduction hear the voice of the bard who present past and future sees whose ears have heard the holy word that walked among the ancient tree calling the lapsed soul and\nSeed word sequence: becomes an infant small he becomes man of woe he doth feel the sorrow too think not thou canst sigh sigh and thy maker is not by think not thou canst weep tear and thy maker is not year oh he gives to us his joy that our grief he may destroy till our grief is fled an gone he doth sit by us and moan songs of experience introduction hear the voice of the bard who present past and future sees whose ears have heard the holy word that walked among the ancient tree calling the lapsed soul and\nPredicted words: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\nBLEU Score for predicted words: 0.2384093783383091\n","output_type":"stream"}]},{"cell_type":"code","source":"model2.fit(X, y, batch_size=50, epochs=100, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T21:42:38.118790Z","iopub.execute_input":"2023-03-26T21:42:38.119170Z","iopub.status.idle":"2023-03-26T22:46:05.824307Z","shell.execute_reply.started":"2023-03-26T21:42:38.119136Z","shell.execute_reply":"2023-03-26T22:46:05.823545Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/100\n130/130 [==============================] - 38s 289ms/step - loss: 6.1773\nEpoch 2/100\n130/130 [==============================] - 37s 285ms/step - loss: 6.1635\nEpoch 3/100\n130/130 [==============================] - 37s 287ms/step - loss: 6.1439\nEpoch 4/100\n130/130 [==============================] - 38s 294ms/step - loss: 6.1223\nEpoch 5/100\n130/130 [==============================] - 38s 294ms/step - loss: 6.0925\nEpoch 6/100\n130/130 [==============================] - 38s 293ms/step - loss: 6.0581\nEpoch 7/100\n130/130 [==============================] - 39s 299ms/step - loss: 6.0205\nEpoch 8/100\n130/130 [==============================] - 39s 297ms/step - loss: 5.9806\nEpoch 9/100\n130/130 [==============================] - 38s 295ms/step - loss: 5.9398\nEpoch 10/100\n130/130 [==============================] - 38s 293ms/step - loss: 5.9032\nEpoch 11/100\n130/130 [==============================] - 38s 294ms/step - loss: 5.8667\nEpoch 12/100\n130/130 [==============================] - 39s 299ms/step - loss: 5.8287\nEpoch 13/100\n130/130 [==============================] - 39s 299ms/step - loss: 5.7926\nEpoch 14/100\n130/130 [==============================] - 39s 300ms/step - loss: 5.7555\nEpoch 15/100\n130/130 [==============================] - 39s 299ms/step - loss: 5.7229\nEpoch 16/100\n130/130 [==============================] - 39s 298ms/step - loss: 5.6905\nEpoch 17/100\n130/130 [==============================] - 38s 294ms/step - loss: 5.6590\nEpoch 18/100\n130/130 [==============================] - 38s 296ms/step - loss: 5.6278\nEpoch 19/100\n130/130 [==============================] - 38s 295ms/step - loss: 5.5995\nEpoch 20/100\n130/130 [==============================] - 38s 294ms/step - loss: 5.5678\nEpoch 21/100\n130/130 [==============================] - 38s 290ms/step - loss: 5.5408\nEpoch 22/100\n130/130 [==============================] - 38s 292ms/step - loss: 5.5076\nEpoch 23/100\n130/130 [==============================] - 38s 294ms/step - loss: 5.4800\nEpoch 24/100\n130/130 [==============================] - 38s 295ms/step - loss: 5.4512\nEpoch 25/100\n130/130 [==============================] - 38s 294ms/step - loss: 5.4216\nEpoch 26/100\n130/130 [==============================] - 38s 292ms/step - loss: 5.3964\nEpoch 27/100\n130/130 [==============================] - 38s 295ms/step - loss: 5.3644\nEpoch 28/100\n130/130 [==============================] - 41s 312ms/step - loss: 5.3351\nEpoch 29/100\n130/130 [==============================] - 39s 302ms/step - loss: 5.3120\nEpoch 30/100\n130/130 [==============================] - 39s 301ms/step - loss: 5.2814\nEpoch 31/100\n130/130 [==============================] - 40s 304ms/step - loss: 5.2545\nEpoch 32/100\n130/130 [==============================] - 39s 298ms/step - loss: 5.2298\nEpoch 33/100\n130/130 [==============================] - 38s 292ms/step - loss: 5.2021\nEpoch 34/100\n130/130 [==============================] - 38s 293ms/step - loss: 5.1739\nEpoch 35/100\n130/130 [==============================] - 39s 302ms/step - loss: 5.1476\nEpoch 36/100\n130/130 [==============================] - 39s 302ms/step - loss: 5.1230\nEpoch 37/100\n130/130 [==============================] - 38s 294ms/step - loss: 5.0939\nEpoch 38/100\n130/130 [==============================] - 39s 298ms/step - loss: 5.0678\nEpoch 39/100\n130/130 [==============================] - 40s 307ms/step - loss: 5.0453\nEpoch 40/100\n130/130 [==============================] - 38s 295ms/step - loss: 5.0154\nEpoch 41/100\n130/130 [==============================] - 38s 295ms/step - loss: 4.9932\nEpoch 42/100\n130/130 [==============================] - 38s 296ms/step - loss: 4.9669\nEpoch 43/100\n130/130 [==============================] - 40s 311ms/step - loss: 5.5546\nEpoch 44/100\n130/130 [==============================] - 40s 304ms/step - loss: 5.1480\nEpoch 45/100\n130/130 [==============================] - 37s 286ms/step - loss: 5.0434\nEpoch 46/100\n130/130 [==============================] - 37s 281ms/step - loss: 4.9917\nEpoch 47/100\n130/130 [==============================] - 37s 284ms/step - loss: 4.9558\nEpoch 48/100\n130/130 [==============================] - 37s 283ms/step - loss: 4.9499\nEpoch 49/100\n130/130 [==============================] - 36s 279ms/step - loss: 4.9040\nEpoch 50/100\n130/130 [==============================] - 36s 280ms/step - loss: 4.8767\nEpoch 51/100\n130/130 [==============================] - 37s 285ms/step - loss: 4.8332\nEpoch 52/100\n130/130 [==============================] - 37s 283ms/step - loss: 4.8608\nEpoch 53/100\n130/130 [==============================] - 37s 284ms/step - loss: 4.7971\nEpoch 54/100\n130/130 [==============================] - 37s 287ms/step - loss: 4.7524\nEpoch 55/100\n130/130 [==============================] - 37s 285ms/step - loss: 4.7357\nEpoch 56/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.7292\nEpoch 57/100\n130/130 [==============================] - 37s 286ms/step - loss: 4.6922\nEpoch 58/100\n130/130 [==============================] - 37s 287ms/step - loss: 4.6524\nEpoch 59/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.6814\nEpoch 60/100\n130/130 [==============================] - 38s 292ms/step - loss: 4.7137\nEpoch 61/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.6849\nEpoch 62/100\n130/130 [==============================] - 37s 288ms/step - loss: 4.6571\nEpoch 63/100\n130/130 [==============================] - 38s 294ms/step - loss: 4.6374\nEpoch 64/100\n130/130 [==============================] - 38s 289ms/step - loss: 4.6224\nEpoch 65/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.5993\nEpoch 66/100\n130/130 [==============================] - 37s 284ms/step - loss: 4.5766\nEpoch 67/100\n130/130 [==============================] - 38s 291ms/step - loss: 4.5601\nEpoch 68/100\n130/130 [==============================] - 37s 288ms/step - loss: 4.5438\nEpoch 69/100\n130/130 [==============================] - 37s 287ms/step - loss: 4.5177\nEpoch 70/100\n130/130 [==============================] - 37s 287ms/step - loss: 4.5016\nEpoch 71/100\n130/130 [==============================] - 38s 291ms/step - loss: 4.4812\nEpoch 72/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.4641\nEpoch 73/100\n130/130 [==============================] - 38s 289ms/step - loss: 4.4450\nEpoch 74/100\n130/130 [==============================] - 38s 293ms/step - loss: 4.4296\nEpoch 75/100\n130/130 [==============================] - 39s 298ms/step - loss: 4.4066\nEpoch 76/100\n130/130 [==============================] - 39s 300ms/step - loss: 4.3880\nEpoch 77/100\n130/130 [==============================] - 39s 301ms/step - loss: 4.3684\nEpoch 78/100\n130/130 [==============================] - 39s 300ms/step - loss: 4.3511\nEpoch 79/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.3342\nEpoch 80/100\n130/130 [==============================] - 37s 286ms/step - loss: 4.3154\nEpoch 81/100\n130/130 [==============================] - 37s 284ms/step - loss: 4.2985\nEpoch 82/100\n130/130 [==============================] - 38s 294ms/step - loss: 4.2784\nEpoch 83/100\n130/130 [==============================] - 39s 299ms/step - loss: 4.2667\nEpoch 84/100\n130/130 [==============================] - 41s 313ms/step - loss: 4.2470\nEpoch 85/100\n130/130 [==============================] - 40s 306ms/step - loss: 4.2351\nEpoch 86/100\n130/130 [==============================] - 39s 300ms/step - loss: 4.2093\nEpoch 87/100\n130/130 [==============================] - 38s 292ms/step - loss: 4.1978\nEpoch 88/100\n130/130 [==============================] - 38s 292ms/step - loss: 4.1788\nEpoch 89/100\n130/130 [==============================] - 37s 288ms/step - loss: 4.1609\nEpoch 90/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.1431\nEpoch 91/100\n130/130 [==============================] - 38s 295ms/step - loss: 4.1312\nEpoch 92/100\n130/130 [==============================] - 37s 285ms/step - loss: 4.1156\nEpoch 93/100\n130/130 [==============================] - 37s 283ms/step - loss: 4.0968\nEpoch 94/100\n130/130 [==============================] - 37s 286ms/step - loss: 4.0786\nEpoch 95/100\n130/130 [==============================] - 38s 292ms/step - loss: 4.0686\nEpoch 96/100\n130/130 [==============================] - 37s 285ms/step - loss: 4.0531\nEpoch 97/100\n130/130 [==============================] - 38s 295ms/step - loss: 4.0425\nEpoch 98/100\n130/130 [==============================] - 38s 290ms/step - loss: 4.0166\nEpoch 99/100\n130/130 [==============================] - 38s 295ms/step - loss: 4.0023\nEpoch 100/100\n130/130 [==============================] - 37s 282ms/step - loss: 3.9865\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fdfcfd71450>"},"metadata":{}}]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model2.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:49:26.826249Z","iopub.execute_input":"2023-03-26T22:49:26.826689Z","iopub.status.idle":"2023-03-26T22:49:33.717368Z","shell.execute_reply.started":"2023-03-26T22:49:26.826654Z","shell.execute_reply":"2023-03-26T22:49:33.716238Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"ah gentle may lay me down and gentle rest my head and gentle sleep the sleep of death and gently hear the voice of him that walketh in the garden in the evening time the lilly of the valley breathing in the humble grass answerd the lovely maid and said am watry weed and am very small and love to dwell in lowly vales so weak the gilded butterfly scarce perches on my head yet am visited from heaven and he that smiles on all walks in the valley and each morn over me spreads his hand saying rejoice thou\n","output_type":"stream"}]},{"cell_type":"code","source":"# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:49:46.552708Z","iopub.execute_input":"2023-03-26T22:49:46.553230Z","iopub.status.idle":"2023-03-26T22:49:52.293861Z","shell.execute_reply.started":"2023-03-26T22:49:46.553183Z","shell.execute_reply":"2023-03-26T22:49:52.292264Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Seed word sequence: ah gentle may lay me down and gentle rest my head and gentle sleep the sleep of death and gently hear the voice of him that walketh in the garden in the evening time the lilly of the valley breathing in the humble grass answerd the lovely maid and said am watry weed and am very small and love to dwell in lowly vales so weak the gilded butterfly scarce perches on my head yet am visited from heaven and he that smiles on all walks in the valley and each morn over me spreads his hand saying rejoice thou\nPredicted words: the meekness of the night the the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the the steeds and the night and the springing and the\nBLEU Score for predicted words: 0.5995532082765309\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#model1 with different paramenters","metadata":{}},{"cell_type":"code","source":"from keras.layers import Reshape\n\nmodel3 = Sequential()\nmodel3.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel3.add(LSTM(128, return_sequences=True))\nmodel3.add(LSTM(64, return_sequences=True))\nmodel3.add(Reshape((1, -1)))\nmodel3.add(LSTM(64))\nmodel3.add(Dense(y.shape[1], activation='softmax'))\nmodel3.compile(loss='categorical_crossentropy', optimizer='adam')\nprint(model3.summary())\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:02:31.666145Z","iopub.execute_input":"2023-03-26T23:02:31.666734Z","iopub.status.idle":"2023-03-26T23:02:32.716834Z","shell.execute_reply.started":"2023-03-26T23:02:31.666681Z","shell.execute_reply":"2023-03-26T23:02:32.715731Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_17 (LSTM)              (None, 100, 128)          66560     \n                                                                 \n lstm_18 (LSTM)              (None, 100, 128)          131584    \n                                                                 \n lstm_19 (LSTM)              (None, 100, 64)           49408     \n                                                                 \n reshape (Reshape)           (None, 1, 6400)           0         \n                                                                 \n lstm_20 (LSTM)              (None, 64)                1655040   \n                                                                 \n dense_2 (Dense)             (None, 1505)              97825     \n                                                                 \n=================================================================\nTotal params: 2,000,417\nTrainable params: 2,000,417\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model3.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)]   ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:02:57.360127Z","iopub.execute_input":"2023-03-26T23:02:57.360675Z","iopub.status.idle":"2023-03-26T23:03:05.758112Z","shell.execute_reply.started":"2023-03-26T23:02:57.360622Z","shell.execute_reply":"2023-03-26T23:03:05.756837Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"more nothing remains maid tell thee when pass away it is to tenfold life to love to peace and raptures holy unseen descending weigh my light wings upon balmy flowers and court the fair eyed dew to take me to her shining tent the weeping virgin trembling kneels before the risen sun till we arise link in golden band and never part but walk united bearing food to all our tender flowers dost thou little cloud fear that am not like thee for walk through the vales of har and smell the sweetest flowers but feed not the little flowers\n","output_type":"stream"}]},{"cell_type":"code","source":"# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:03:24.774893Z","iopub.execute_input":"2023-03-26T23:03:24.775269Z","iopub.status.idle":"2023-03-26T23:03:30.539533Z","shell.execute_reply.started":"2023-03-26T23:03:24.775224Z","shell.execute_reply":"2023-03-26T23:03:30.538156Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Seed word sequence: more nothing remains maid tell thee when pass away it is to tenfold life to love to peace and raptures holy unseen descending weigh my light wings upon balmy flowers and court the fair eyed dew to take me to her shining tent the weeping virgin trembling kneels before the risen sun till we arise link in golden band and never part but walk united bearing food to all our tender flowers dost thou little cloud fear that am not like thee for walk through the vales of har and smell the sweetest flowers but feed not the little flowers\nPredicted words: afar afar afar snare snare snare arm arm arm arm arm arm arm arm arm arm vagabond vagabond vagabond vagabond vagabond vagabond vagabond vagabond hears hears springing springing springing springing springing springing springing springing springing arm arm arm arm arm arm arm to to to to to snare to to to to snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare snare vagabond vagabond vagabond vagabond vagabond vagabond snare snare snare snare snare snare snare snare snare snare snare snare\nBLEU Score for predicted words: 0.35792639299738777\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.layers import Dropout\n\nmodel4 = Sequential()\nmodel4.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel4.add(Dropout(0.2))\nmodel4.add(LSTM(128, return_sequences=True))\nmodel4.add(Dropout(0.2))\nmodel4.add(LSTM(64))\nmodel4.add(Dropout(0.2))\nmodel4.add(Dense(y.shape[1], activation='softmax'))\n\nmodel4.compile(loss='categorical_crossentropy', optimizer='adam')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:06:22.542132Z","iopub.execute_input":"2023-03-26T23:06:22.542660Z","iopub.status.idle":"2023-03-26T23:06:23.352271Z","shell.execute_reply.started":"2023-03-26T23:06:22.542589Z","shell.execute_reply":"2023-03-26T23:06:23.350175Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model4.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)] ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:06:48.625468Z","iopub.execute_input":"2023-03-26T23:06:48.627015Z","iopub.status.idle":"2023-03-26T23:06:58.142516Z","shell.execute_reply.started":"2023-03-26T23:06:48.626926Z","shell.execute_reply":"2023-03-26T23:06:58.141419Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"sorrow and care dismay how shall the summer arise in joy or the summer fruits appear or how shall we gather what griefs destroy or bless the mellowing year when the blasts of winter appear to terzah whate er is born of mortal birth must be consumed with the earth to rise from generation free then what have to do with thee the sexes sprang from shame and pride blown in the morn in evening died but mercy changed death into sleep the sexes rose to work and weep thou mother of my mortal part with cruelty didst mould my\n","output_type":"stream"}]},{"cell_type":"code","source":"# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:07:11.791128Z","iopub.execute_input":"2023-03-26T23:07:11.791599Z","iopub.status.idle":"2023-03-26T23:07:17.135197Z","shell.execute_reply.started":"2023-03-26T23:07:11.791552Z","shell.execute_reply":"2023-03-26T23:07:17.133605Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Seed word sequence: sorrow and care dismay how shall the summer arise in joy or the summer fruits appear or how shall we gather what griefs destroy or bless the mellowing year when the blasts of winter appear to terzah whate er is born of mortal birth must be consumed with the earth to rise from generation free then what have to do with thee the sexes sprang from shame and pride blown in the morn in evening died but mercy changed death into sleep the sexes rose to work and weep thou mother of my mortal part with cruelty didst mould my\nPredicted words: curtain curtain arrow arrow arrow arrow arrow arrow arrow arrow arrow curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain\nBLEU Score for predicted words: 0.12357205939138162\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras import regularizers\n\nmodel5 = Sequential()\nmodel5.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True, kernel_regularizer=regularizers.l2(0.01)))\nmodel5.add(LSTM(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)))\nmodel5.add(LSTM(64, kernel_regularizer=regularizers.l2(0.01)))\nmodel5.add(Dense(y.shape[1], activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:08:30.928680Z","iopub.execute_input":"2023-03-26T23:08:30.929080Z","iopub.status.idle":"2023-03-26T23:08:31.650919Z","shell.execute_reply.started":"2023-03-26T23:08:30.929047Z","shell.execute_reply":"2023-03-26T23:08:31.649692Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model4.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)] \n# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:09:09.808718Z","iopub.execute_input":"2023-03-26T23:09:09.809106Z","iopub.status.idle":"2023-03-26T23:09:22.233655Z","shell.execute_reply.started":"2023-03-26T23:09:09.809072Z","shell.execute_reply":"2023-03-26T23:09:22.232259Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"now like mighty wild they raise to heaven the voice of song or like harmonious thunderings the seats of heaven among beneath them sit the aged man wise guardians of the poor then cherish pity lest you drive an angel from your door night the sun descending in the west the evening star does shine the birds are silent in their nest and must seek for mine the moon like flower in heaven high bower with silent delight sits and smiles on the night farewell green fields and happy grove where flocks have ta en delight where lambs have nibbled\nSeed word sequence: now like mighty wild they raise to heaven the voice of song or like harmonious thunderings the seats of heaven among beneath them sit the aged man wise guardians of the poor then cherish pity lest you drive an angel from your door night the sun descending in the west the evening star does shine the birds are silent in their nest and must seek for mine the moon like flower in heaven high bower with silent delight sits and smiles on the night farewell green fields and happy grove where flocks have ta en delight where lambs have nibbled\nPredicted words: curtain curtain curtain arrow arrow arrow arrow arrow arrow arrow curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain curtain\nBLEU Score for predicted words: 0.11877024342309944\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#model5","metadata":{}},{"cell_type":"markdown","source":"#model6","metadata":{}},{"cell_type":"code","source":"model5 = Sequential()\n# LSTM layer has 800 neurons (units).  The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\nmodel5.add(LSTM(150, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel5.add(LSTM(140, return_sequences=True))\nmodel5.add(LSTM(128))\nmodel5.add(Dense(y.shape[1], activation='softmax'))\n\nmodel5.summary()\n\n\n\nmodel5.compile(loss='categorical_crossentropy', optimizer='adam')\n#print(model2.summary())\nmodel5.fit(X, y, batch_size=50, epochs=50, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T23:10:54.819361Z","iopub.execute_input":"2023-03-26T23:10:54.819749Z","iopub.status.idle":"2023-03-26T23:53:05.068796Z","shell.execute_reply.started":"2023-03-26T23:10:54.819714Z","shell.execute_reply":"2023-03-26T23:53:05.067650Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Model: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_27 (LSTM)              (None, 100, 150)          91200     \n                                                                 \n lstm_28 (LSTM)              (None, 100, 140)          162960    \n                                                                 \n lstm_29 (LSTM)              (None, 128)               137728    \n                                                                 \n dense_5 (Dense)             (None, 1505)              194145    \n                                                                 \n=================================================================\nTotal params: 586,033\nTrainable params: 586,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n130/130 [==============================] - 52s 366ms/step - loss: 6.5416\nEpoch 2/50\n130/130 [==============================] - 49s 373ms/step - loss: 6.2055\nEpoch 3/50\n130/130 [==============================] - 47s 364ms/step - loss: 6.1852\nEpoch 4/50\n130/130 [==============================] - 48s 367ms/step - loss: 6.1790\nEpoch 5/50\n130/130 [==============================] - 48s 367ms/step - loss: 6.1746\nEpoch 6/50\n130/130 [==============================] - 48s 366ms/step - loss: 6.1700\nEpoch 7/50\n130/130 [==============================] - 48s 372ms/step - loss: 6.1694\nEpoch 8/50\n130/130 [==============================] - 49s 376ms/step - loss: 6.1678\nEpoch 9/50\n130/130 [==============================] - 49s 378ms/step - loss: 6.1644\nEpoch 10/50\n130/130 [==============================] - 48s 369ms/step - loss: 6.1634\nEpoch 11/50\n130/130 [==============================] - 49s 373ms/step - loss: 6.1624\nEpoch 12/50\n130/130 [==============================] - 50s 386ms/step - loss: 6.1598\nEpoch 13/50\n130/130 [==============================] - 50s 388ms/step - loss: 6.1585\nEpoch 14/50\n130/130 [==============================] - 49s 381ms/step - loss: 6.1557\nEpoch 15/50\n130/130 [==============================] - 50s 385ms/step - loss: 6.1528\nEpoch 16/50\n130/130 [==============================] - 49s 375ms/step - loss: 6.1498\nEpoch 17/50\n130/130 [==============================] - 51s 395ms/step - loss: 6.1469\nEpoch 18/50\n130/130 [==============================] - 50s 384ms/step - loss: 6.1416\nEpoch 19/50\n130/130 [==============================] - 49s 374ms/step - loss: 6.1315\nEpoch 20/50\n130/130 [==============================] - 49s 380ms/step - loss: 6.1095\nEpoch 21/50\n130/130 [==============================] - 49s 380ms/step - loss: 6.0736\nEpoch 22/50\n130/130 [==============================] - 49s 380ms/step - loss: 6.0364\nEpoch 23/50\n130/130 [==============================] - 51s 389ms/step - loss: 5.9864\nEpoch 24/50\n130/130 [==============================] - 53s 405ms/step - loss: 5.9239\nEpoch 25/50\n130/130 [==============================] - 52s 396ms/step - loss: 5.8658\nEpoch 26/50\n130/130 [==============================] - 52s 397ms/step - loss: 5.7963\nEpoch 27/50\n130/130 [==============================] - 51s 395ms/step - loss: 5.7358\nEpoch 28/50\n130/130 [==============================] - 51s 394ms/step - loss: 5.6698\nEpoch 29/50\n130/130 [==============================] - 50s 384ms/step - loss: 5.6111\nEpoch 30/50\n130/130 [==============================] - 53s 409ms/step - loss: 5.5520\nEpoch 31/50\n130/130 [==============================] - 52s 396ms/step - loss: 5.4893\nEpoch 32/50\n130/130 [==============================] - 52s 403ms/step - loss: 5.4262\nEpoch 33/50\n130/130 [==============================] - 51s 392ms/step - loss: 5.3652\nEpoch 34/50\n130/130 [==============================] - 52s 397ms/step - loss: 5.3077\nEpoch 35/50\n130/130 [==============================] - 52s 399ms/step - loss: 5.2457\nEpoch 36/50\n130/130 [==============================] - 51s 396ms/step - loss: 5.1849\nEpoch 37/50\n130/130 [==============================] - 51s 391ms/step - loss: 5.1282\nEpoch 38/50\n130/130 [==============================] - 52s 403ms/step - loss: 5.0665\nEpoch 39/50\n130/130 [==============================] - 52s 397ms/step - loss: 5.0091\nEpoch 40/50\n130/130 [==============================] - 53s 404ms/step - loss: 4.9515\nEpoch 41/50\n130/130 [==============================] - 53s 406ms/step - loss: 4.8973\nEpoch 42/50\n130/130 [==============================] - 51s 395ms/step - loss: 4.8412\nEpoch 43/50\n130/130 [==============================] - 51s 395ms/step - loss: 4.7873\nEpoch 44/50\n130/130 [==============================] - 51s 394ms/step - loss: 4.7329\nEpoch 45/50\n130/130 [==============================] - 52s 397ms/step - loss: 4.6805\nEpoch 46/50\n130/130 [==============================] - 52s 403ms/step - loss: 4.6266\nEpoch 47/50\n130/130 [==============================] - 53s 410ms/step - loss: 4.5770\nEpoch 48/50\n130/130 [==============================] - 52s 400ms/step - loss: 4.5255\nEpoch 49/50\n130/130 [==============================] - 51s 394ms/step - loss: 4.4718\nEpoch 50/50\n130/130 [==============================] - 52s 397ms/step - loss: 4.4215\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fdfce7201d0>"},"metadata":{}}]},{"cell_type":"code","source":"# Make Predictions\nrandom_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\nrandom_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n\nindex_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\nseed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n\n# join the words in the list and print the sequence of words\nprint(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n\n# Predict next 100 words\nword_sequence = []\nfor i in range(100):\n    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n\n    predicted_word_index = model5.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n\n    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n    random_seq = random_seq[1:len(random_seq)] \n# BLEU score\nseq = [' '.join(w) for w in input_sequence_words]\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = seq\ncandidate = ' '.join(word_sequence) # make the list of words into a string\nscore = sentence_bleu(reference, candidate)\nprint('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\nprint('Predicted words: %s'%(candidate))\nprint('BLEU Score for predicted words: %s'%(score))","metadata":{"execution":{"iopub.status.busy":"2023-03-27T00:00:34.668835Z","iopub.execute_input":"2023-03-27T00:00:34.669213Z","iopub.status.idle":"2023-03-27T00:00:48.854656Z","shell.execute_reply.started":"2023-03-27T00:00:34.669177Z","shell.execute_reply":"2023-03-27T00:00:48.853538Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"merrily to welcome in the year little boy full of joy little girl sweet and small cock does crow so do you merry voice infant noise merrily merrily to welcome in the year little lamb here am come and lick my white neck let me pull your soft wool let me kiss your soft face merrily merrily to welcome in the year nurse song when the voices of children are heard on the green and laughing is heard on the hill my heart is at rest within my breast and everything else is still then come home my children the\nSeed word sequence: merrily to welcome in the year little boy full of joy little girl sweet and small cock does crow so do you merry voice infant noise merrily merrily to welcome in the year little lamb here am come and lick my white neck let me pull your soft wool let me kiss your soft face merrily merrily to welcome in the year nurse song when the voices of children are heard on the green and laughing is heard on the hill my heart is at rest within my breast and everything else is still then come home my children the\nPredicted words: the evening and the sit and the chorus and the the the the smiles and the the the shepherd thee the the dews and the the the gather in the the the the smiles and the the the shepherd thee the the dews and the the the the smiles and the the the shepherd thee the the dews and the the the the smiles and the the the shepherd thee the the dews and the the the the smiles and the the the shepherd thee the the dews and the the the the smiles and the the the shepherd thee\nBLEU Score for predicted words: 0.6393819639846364\n","output_type":"stream"}]},{"cell_type":"code","source":"#GRU\n#Third model\n\nmodel = Sequential()\n# GRU layer has 800 neurons (units). The input shape is (100, 1) (Number of words in a sequence, 1 to make it 2D data) (Number of time-steps, features per time-step)\nmodel.add(GRU(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(GRU(800, return_sequences=True))\nmodel.add(GRU(800))\nmodel.add(Dense(y.shape[1], activation='softmax'))\n\nmodel.summary()\n\n# the output word can be one of any of the unique words in the vocabulary\n# This means it is a multi-class calssification problem and we use the categorical crossentropy loss function\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{},"execution_count":null,"outputs":[]}]}